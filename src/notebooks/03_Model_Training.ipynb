{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0138105-9c18-41e4-a40d-04f0a29654ab",
   "metadata": {},
   "source": [
    "# Model Training - Fraud Detection with XGBoost\n",
    "\n",
    "In this notebook, we will train and evaluate **machine learning models** for credit card fraud detection.\n",
    "\n",
    "## **Steps We Will Cover**\n",
    "1. **Load Preprocessed Data** → Use **both** the original & SMOTE-balanced datasets.  \n",
    "2. **Train-Test Split** → Prepare data for model training.  \n",
    "3. **Train Baseline XGBoost Model** → Evaluate initial performance.  \n",
    "4. **Optimize Model Performance** → Use **Optuna** for hyperparameter tuning.  \n",
    "5. **Evaluate Model Performance** → Check metrics like **AUC-ROC, Precision-Recall**.\n",
    "\n",
    "Our goal is to **build an accurate fraud detection system** while handling the **highly imbalanced dataset** properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a4bfa-7447-4263-a5a9-829ffab3fab2",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "\n",
    "Before training our models, we first import essential libraries for:\n",
    "- **Data Handling & Processing** → `pandas`, `numpy`\n",
    "- **Model Training** → `XGBoost`, `Scikit-Learn`\n",
    "- **Hyperparameter Optimization** → `Optuna`\n",
    "- **Evaluation Metrics** → `classification_report`, `roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cecdb8-c6f0-44a0-8991-52432b25f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning Models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Selection & Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9bc33-bb47-49ef-82ed-a95afa016cdc",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data & Train-Test Split\n",
    "\n",
    "To train our fraud detection model, we will:\n",
    "1. **Load the preprocessed datasets**:\n",
    "   - `X_scaled.csv` → Scaled but imbalanced dataset.\n",
    "   - `X_smote.csv` → SMOTE-balanced dataset.\n",
    "2. **Perform Train-Test Split**:\n",
    "   - **80% training, 20% testing**.\n",
    "   - Ensure data is shuffled properly before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2502952a-b037-4279-8eaf-cf0c6a80feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Shape:\n",
      "Train: (227845, 30), Test: (56962, 30)\n",
      "\n",
      "SMOTE Dataset Shape:\n",
      "Train: (454904, 30), Test: (113726, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed datasets\n",
    "X_scaled = pd.read_csv(\"../datasets/X_scaled.csv\")\n",
    "y = pd.read_csv(\"../datasets/y.csv\")\n",
    "\n",
    "X_smote = pd.read_csv(\"../datasets/X_smote.csv\")\n",
    "y_smote = pd.read_csv(\"../datasets/y_smote.csv\")\n",
    "\n",
    "# Ensure y is a 1D array (avoid shape issues)\n",
    "y = y.values.ravel()\n",
    "y_smote = y_smote.values.ravel()\n",
    "\n",
    "# Train-Test Split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42, stratify=y_smote)\n",
    "\n",
    "# Display dataset shapes\n",
    "print(\"Original Dataset Shape:\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nSMOTE Dataset Shape:\")\n",
    "print(f\"Train: {X_train_smote.shape}, Test: {X_test_smote.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d50fff-d2cf-481f-a28b-90d47f29afd5",
   "metadata": {},
   "source": [
    "## Training Three XGBoost Models\n",
    "\n",
    "To compare different approaches, we will train three XGBoost models:\n",
    "\n",
    "️1. **Base XGBoost Model** → Trained on the **original imbalanced dataset** without any adjustments.  \n",
    "2. **Weighted XGBoost Model** → Trained on the **original dataset**, but with **higher penalty for misclassifying fraud cases** using `scale_pos_weight`.  \n",
    "3. **SMOTE XGBoost Model** → Trained on the **SMOTE-balanced dataset**, where fraud cases are oversampled to match non-fraud cases.  \n",
    "\n",
    "This will help us determine:\n",
    "- Whether **handling imbalance improves fraud detection**.  \n",
    "- Whether **penalizing fraud misclassification improves model focus**.  \n",
    "- Which **approach provides the best overall fraud detection performance**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8c472e-6649-4e4c-b9a1-53368a4f3625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base XGBoost Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.89      0.81      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.94      0.90      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "AUC-ROC Score: 0.9030\n"
     ]
    }
   ],
   "source": [
    "# Initialize Base XGBoost Model\n",
    "base_xgb = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42, device='gpu')\n",
    "\n",
    "# Train on imbalanced dataset\n",
    "base_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_base = base_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Base XGBoost Model Performance:\")\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_pred_base):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73e800e-c722-4907-972c-b2c30520b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted XGBoost Model Performance (Higher Fraud Penalty):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.88      0.83      0.85        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.94      0.91      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "AUC-ROC Score: 0.9132\n"
     ]
    }
   ],
   "source": [
    "# Calculate the imbalance ratio (weighting fraud more)\n",
    "fraud_weight = len(y_train) / sum(y_train)  # Gives more weight to minority class\n",
    "\n",
    "# Initialize Weighted XGBoost Model\n",
    "weighted_xgb = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", scale_pos_weight=fraud_weight, random_state=42, device=\"gpu\")\n",
    "\n",
    "# Train with class weighting\n",
    "weighted_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_weighted = weighted_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Weighted XGBoost Model Performance (Higher Fraud Penalty):\")\n",
    "print(classification_report(y_test, y_pred_weighted))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_pred_weighted):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728b392f-69a4-47c8-9f2f-cf5cd7fe142e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE XGBoost Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56863\n",
      "           1       1.00      1.00      1.00     56863\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "AUC-ROC Score: 0.9996\n"
     ]
    }
   ],
   "source": [
    "# Initialize SMOTE XGBoost Model\n",
    "smote_xgb = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42, device=\"gpu\")\n",
    "\n",
    "# Train on SMOTE-balanced dataset\n",
    "smote_xgb.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_smote = smote_xgb.predict(X_test_smote)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"SMOTE XGBoost Model Performance:\")\n",
    "print(classification_report(y_test_smote, y_pred_smote))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test_smote, y_pred_smote):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa15c9-4987-47fe-bcb1-954d58406c55",
   "metadata": {},
   "source": [
    "## Model Performance Comparison\n",
    "\n",
    "After training three different XGBoost models, we can compare their effectiveness in fraud detection.\n",
    "\n",
    "| Model | Precision (Fraud) | Recall (Fraud) | F1-Score (Fraud) | AUC-ROC Score |\n",
    "|---|---|---|---|---|\n",
    "| **Base XGBoost** | **0.89** | **0.81** | **0.84** | **0.9030** |\n",
    "| **Weighted XGBoost (Higher Fraud Penalty)** | **0.88** | **0.83** | **0.85** | **0.9132** |\n",
    "| **SMOTE XGBoost** | **1.00** | **1.00** | **1.00** | **0.9996** |\n",
    "\n",
    "### **Observations**\n",
    "- **Base XGBoost Model** performed well but had slightly **lower recall (81%)**, meaning it **missed** some fraud cases.\n",
    "- **Weighted XGBoost Model** (higher fraud misclassification penalty) **slightly improved recall (83%)**, making it **better at detecting fraud** while maintaining high precision.\n",
    "- **SMOTE XGBoost Model** achieved **perfect scores**, but this is likely **overfitting**, as the dataset was fully balanced by oversampling fraud cases.\n",
    "\n",
    "### **Key Takeaways**\n",
    "✔ **Weighted XGBoost improved recall while maintaining precision** → A better model for fraud detection.  \n",
    "✔ **SMOTE Model may be overfitting** due to the perfectly balanced dataset.  \n",
    "✔ **Base Model is strong but could miss some fraud cases** (lower recall).  \n",
    "\n",
    "### **Next Steps**\n",
    "1. **Use SHAP for feature importance analysis** → Understand which features impact predictions.  \n",
    "2. **Hyperparameter tuning with Optuna** → Improve model performance further.  \n",
    "3. **Final model selection** → Choose the best approach for real-world fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270da4c-65a3-4129-a665-ff41a393888e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
